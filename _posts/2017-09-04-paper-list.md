---
layout: post
title: SGD论文列表
date: '2017-09-04 17:05:13 +0000'
categories: jekyll update
published: true
--- 

随机梯度（包括降方差梯度下降）下降相关论文：


* **Stochastic Averaged Gradient**:
	* Roux, Nicolas L., Mark Schmidt, and Francis R. Bach. "*A stochastic gradient method with an exponential convergence rate for finite training sets.*" Advances in Neural Information Processing Systems. 2012.

* **Stochastic Dual Coordinate Ascent**
	* Shalev-Shwartz, Shai, and Tong Zhang. "*Stochastic dual coordinate ascent methods for regularized loss minimization.*" Journal of Machine Learning Research 14.Feb (2013): 567-599.
	* Zhao, Peilin, and Tong Zhang. "Stochastic optimization with importance sampling for regularized loss minimization." Proceedings of the 32nd International Conference on Machine Learning (ICML-15). 2015.

* **Stochastic Variance Reduced Gradient** descent:
	* Johnson, Rie, and Tong Zhang. "*Accelerating stochastic gradient descent using predictive variance reduction.*" Advances in neural information processing systems. 2013.
	* Xiao, Lin, and Tong Zhang. "*A proximal stochastic gradient method with progressive variance reduction.*" SIAM Journal on Optimization 24.4 (2014): 2057-2075.

* **Mixed Optimization**
	* Mahdavi, Mehrdad, Lijun Zhang, and Rong Jin. "*Mixed optimization for smooth functions.*" Advances in Neural Information Processing Systems. 2013.

* **SAGA**
	* Defazio, Aaron, Francis Bach, and Simon Lacoste-Julien. "*Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.*" Advances in Neural Information Processing Systems. 2014.

深度学习优化相关论文：

