---
layout: post
title: SGD论文列表
date: '2017-09-04 17:05:13 +0000'
categories: jekyll update
published: true
--- 

随机梯度（包括降方差梯度下降）下降相关论文：

* **Stochastic Averaged Gradient**:
	* **[roux2012stochastic]** Roux, Nicolas L., Mark Schmidt, and Francis R. Bach. "*A stochastic gradient method with an exponential convergence rate for finite training sets.*" Advances in Neural Information Processing Systems. 2012.

* **Stochastic Dual Coordinate Ascent**:
	* **[shalev2013stochastic]** Shalev-Shwartz, Shai, and Tong Zhang. "*Stochastic dual coordinate ascent methods for regularized loss minimization.*" Journal of Machine Learning Research 14.Feb (2013): 567-599.
	* **[zhao2015stochastic]** Zhao, Peilin, and Tong Zhang. "*Stochastic optimization with importance sampling for regularized loss minimization.*" Proceedings of the 32nd International Conference on Machine Learning (ICML-15). 2015.
	* **[csiba2015stochastic]** Csiba, Dominik, Zheng Qu, and Peter Richtárik. "*Stochastic dual coordinate ascent with adaptive probabilities.*" Proceedings of the 32nd International Conference on Machine Learning (ICML-15). 2015.

* **Stochastic Variance Reduced Gradient** descent:
	* **[johnson2013accelerating]** Johnson, Rie, and Tong Zhang. "*Accelerating stochastic gradient descent using predictive variance reduction.*" Advances in neural information processing systems. 2013.
	* **[xiao2014proximal]** Xiao, Lin, and Tong Zhang. "*A proximal stochastic gradient method with progressive variance reduction.*" SIAM Journal on Optimization 24.4 (2014): 2057-2075.

* **Mixed Optimization**:
	* **[mahdavi2013mixed]** Mahdavi, Mehrdad, Lijun Zhang, and Rong Jin. "*Mixed optimization for smooth functions.*" Advances in Neural Information Processing Systems. 2013.

* **SAGA**:
	* **[defazio2014saga]** Defazio, Aaron, Francis Bach, and Simon Lacoste-Julien. "*Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.*" Advances in Neural Information Processing Systems. 2014.

* **Accelerated Algorithm**:
	* **[frostig2015regularizing]** Frostig, Roy, et al. "*Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization.*" International Conference on Machine Learning. 2015.
	* **[lin2015universal]** Lin, Hongzhou, Julien Mairal, and Zaid Harchaoui. "*A universal catalyst for first-order optimization.*" Advances in Neural Information Processing Systems. 2015.
	* **[allen2016katyusha]** Allen-Zhu, Zeyuan. "*Katyusha: Accelerated Variance Reduction for Faster SGD.*" arXiv preprint arXiv:1603.05953 (2016).

深度学习优化相关论文：
* **Adam**:
	* **[kingma2014adam]** Kingma, Diederik, and Jimmy Ba. "*Adam: A method for stochastic optimization.*" arXiv preprint arXiv:1412.6980 (2014).

* **Neural Networks: Tricks of the trade**:
	* **[bengio2012practical]** Bengio, Yoshua. "*Practical recommendations for gradient-based training of deep architectures.*" Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 437-478.

